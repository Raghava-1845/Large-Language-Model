# -*- coding: utf-8 -*-
"""tokenization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BXIVBDh3M5KK7vdjzMqdlp8s7CDqZT68
"""

text = "I am Raghava\ntext"
tokens = text.split()   # splits on any whitespace
print(tokens)

text = "Raghava is lazy"
tokens = list(text)
print(tokens)

text = "Tokenization with stop word removal is very useful in NLP tasks"

stop_words = {"is", "with", "in", "the", "and", "a", "an"}

tokens = text.lower().split()
filtered_tokens = [w for w in tokens if w not in stop_words]

print(filtered_tokens)

text = "hey google!.This is Raghava.I am a arrogant boy."

sentences = text.split(".")


sentences = [s.strip() for s in sentences if s.strip()]

print("Sentences:")
for i, s in enumerate(sentences, 1):
    print(f"{i}. {s}")

print("\nWord Tokens in Each Sentence:")
for i, s in enumerate(sentences, 1):
    words = s.split()
    print(f"Sentence {i} words:", words)

text = "I am Raghava.\nThis is word tokenisation!"

# Tokenise on whitespace
tokens = text.split()

print(tokens)

import nltk
nltk.download('punkt_tab')

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

# Download required resources (run once)
nltk.download('punkt')
nltk.download('stopwords')

# Given text
text = "Raghava's favourite player is Kohli"

# Word tokenization
words = word_tokenize(text)

# Load English stopwords
stop_words = set(stopwords.words('english'))

# Remove stopwords
filtered_words = [word for word in words if word.lower() not in stop_words]

print("Original Tokens:")
print(words)

print("\nAfter Stop Word Removal:")
print(filtered_words)

from sklearn.feature_extraction.text import CountVectorizer

# Sample text data
documents = [
    "Raghava likes cricket",
    "Kohli plays cricket",
    "Raghava likes Kohli"
]

# Create Bag of Words model
vectorizer = CountVectorizer()

# Fit and transform the documents
bow_matrix = vectorizer.fit_transform(documents)

# Convert to array
bow_array = bow_matrix.toarray()

# Get feature names (vocabulary)
features = vectorizer.get_feature_names_out()

print("Vocabulary:")
print(features)

print("\nBag of Words Matrix:")
print(bow_array)

!pip install gensim

from gensim.models import Word2Vec

# Sample sentences (tokenized)
sentences = [
    ["raghava", "likes", "cricket"],
    ["kohli", "plays", "cricket"],
    ["raghava", "likes", "kohli"]
]

# Train Word2Vec model
model = Word2Vec(
    sentences,
    vector_size=100,   # size of word vectors
    window=5,          # context window
    min_count=1,       # include all words
    sg=0               # 0 = CBOW, 1 = Skip-gram
)

# Get vector for a word
vector = model.wv["raghava"]

print("Word Vector for 'raghava':")
print(vector)

# Similar words
print("\nWords similar to 'raghava':")
print(model.wv.most_similar("raghava"))

import gensim.downloader as api

# Load pre-trained GloVe model (100-dimensional)
glove_model = api.load("glove-wiki-gigaword-100")

# Get vector for a word
vector = glove_model["cricket"]

print("Vector for 'cricket':")
print(vector)

# Find similar words
print("\nSimilar words to 'cricket':")
print(glove_model.most_similar("cricket"))

!pip install transformers torch

from transformers import BertTokenizer, BertModel
import torch

# Load pre-trained BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")

# Input text
text = "Raghava likes cricket"

# Tokenize input
inputs = tokenizer(text, return_tensors="pt")

# Get embeddings
with torch.no_grad():
    outputs = model(**inputs)

# Last hidden state (word embeddings)
last_hidden_states = outputs.last_hidden_state

print("BERT Embedding Shape:")
print(last_hidden_states.shape)